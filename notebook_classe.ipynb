{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "657f5a27-bb25-44c4-920e-1a80cab199c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyiris.ingestion.config.file_system_config import FileSystemConfig\n",
    "from pyiris.ingestion.extract import ExtractService, FileReader\n",
    "from pyiris.ingestion.load import LoadService, FileWriter\n",
    "from pyiris.infrastructure import Spark\n",
    "\n",
    "import pyspark.sql.functions as f \n",
    "from pyspark.sql import  Row, Window\n",
    "\n",
    "import pyspark as ps\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV,cross_validate\n",
    "from scipy.stats import uniform, randint, loguniform #,quniform\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error,make_scorer,mean_absolute_error, mean_squared_error,r2_score\n",
    "\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "# from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,LabelEncoder\n",
    "from sklearn.base import clone\n",
    "\n",
    "import hyperopt\n",
    "from hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK, Trials\n",
    " \n",
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# import pymer4\n",
    "# from pymer4.models import Lmer\n",
    "\n",
    "from datetime import datetime\n",
    "import os, sys\n",
    "import json\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.ERROR)\n",
    "pyiris_spark = Spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aae3b75-4403-46fa-b9cc-c1c8030165bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_base_lake(file: str) -> ps.sql.dataframe.DataFrame:\n",
    "    \"\"\"\n",
    "    LÃª uma base de dados do data lake e retorna um DataFrame.\n",
    "    ---\n",
    "    file (str): Nome do arquivo a ser lido.\n",
    "    ---\n",
    "    return:\n",
    "    base (ps.sql.dataframe.DataFrame): DataFrame com os dados lidos do data lake.\n",
    "    \"\"\"\n",
    "    path_to_file = (r\"Sales/InteligenciaDeMercado/\" + file)\n",
    "    base = (\n",
    "        FileReader(table_id=file.lower(),\n",
    "                   data_lake_zone='consumezone',\n",
    "                   country='Brazil',\n",
    "                   path=path_to_file,\n",
    "                   format='parquet')\n",
    "        .consume(spark=pyiris_spark)\n",
    "    )\n",
    "    return base\n",
    " \n",
    "def write_lake(base: ps.sql.dataframe.DataFrame, file: str) -> None:\n",
    "    \"\"\"\n",
    "    Escreve um DataFrame no data lake.\n",
    "    ---\n",
    "    base (ps.sql.dataframe.DataFrame): DataFrame a ser salvo no data lake.\n",
    "    file (str): Nome do arquivo para salvar.\n",
    "    ---\n",
    "    return: None\n",
    "    \"\"\"\n",
    "    path_to_file = (r\"Sales/InteligenciaDeMercado/\" + file)\n",
    "    file_config = FileSystemConfig(\n",
    "        mount_name='consumezone',\n",
    "        country='Brazil',\n",
    "        path=path_to_file,\n",
    "        mode='overwrite',\n",
    "        format='parquet'\n",
    "    )\n",
    "    writer = [FileWriter(config=file_config)]\n",
    "    load_service = LoadService(writers=writer)\n",
    "    load_service.commit(dataframe=base)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41174f53-e91a-4ff2-8a10-e314f567019f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from scipy.special import gammaln  # for Poisson and Gamma likelihoods\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------\n",
    "# Helper: Inverse Link Functions\n",
    "# -----------------------\n",
    "def inverse_link(eta, link):\n",
    "    if link == 'identity':\n",
    "        return eta\n",
    "    elif link == 'log':\n",
    "        return torch.exp(eta)\n",
    "    elif link == 'logit':\n",
    "        return torch.sigmoid(eta)\n",
    "    else:\n",
    "        raise ValueError(\"Link not recognized.\")\n",
    "\n",
    "# -----------------------\n",
    "# Helper: Log-likelihood functions for different distributions\n",
    "# -----------------------\n",
    "def loglik_gaussian(y, mu, sigma_y):\n",
    "    ll = -0.5 * torch.log(2 * math.pi * sigma_y*2) - 0.5 * ((y - mu)2) / (sigma_y*2)\n",
    "    return ll\n",
    "\n",
    "def loglik_binomial(y, mu):\n",
    "    eps = 1e-6\n",
    "    mu = torch.clamp(mu, eps, 1 - eps)\n",
    "    ll = y * torch.log(mu) + (1 - y) * torch.log(1 - mu)\n",
    "    return ll\n",
    "\n",
    "def loglik_poisson(y, mu):\n",
    "    ll = -mu + y * torch.log(mu) - torch.lgamma(y + 1)\n",
    "    return ll\n",
    "\n",
    "def loglik_gamma(y, mu, phi):\n",
    "    # Parameterization: E(y)=mu, shape=phi.\n",
    "    eps = 1e-6\n",
    "    mu = torch.clamp(mu, eps, None)\n",
    "    ll = (phi * torch.log(phi/mu) - torch.lgamma(phi)) + (phi - 1)*torch.log(y) - (phi*y/mu)\n",
    "    return ll\n",
    "\n",
    "# -----------------------\n",
    "# Helper: Parse R-style Formula\n",
    "# -----------------------\n",
    "def parse_formula(formula, data):\n",
    "    \"\"\"\n",
    "    Parse a simple R-style formula.\n",
    "    Expected form: \"y ~ x1 + x2 + (1 + x2 | group)\"\n",
    "    \n",
    "    Returns:\n",
    "      X: fixed effects design matrix as numpy array (including an intercept)\n",
    "      y: response vector (numpy array)\n",
    "      groups: grouping variable (numpy array)\n",
    "      random_effect_cols: list of indices (in X) corresponding to random slopes.\n",
    "          (Index 0 is reserved for the intercept.)\n",
    "      fixed_colnames: list of column names in X.\n",
    "    \"\"\"\n",
    "    # Split formula at '~'\n",
    "    lhs, rhs = formula.split(\"~\")\n",
    "    response = lhs.strip()\n",
    "    rhs = rhs.strip()\n",
    "    \n",
    "    # Find random effect terms: look for patterns like \"( ... | ... )\"\n",
    "    rand_term = None\n",
    "    rand_pattern = r\"\\((.*?)\\)\"\n",
    "    m = re.search(rand_pattern, rhs)\n",
    "    if m:\n",
    "        rand_term = m.group(1)  # e.g., \"1 + x2 | group\"\n",
    "        # Remove the random term from rhs\n",
    "        rhs = re.sub(rand_pattern, \"\", rhs).strip()\n",
    "    \n",
    "    # Fixed effects: split remaining rhs by '+' and remove any empty parts\n",
    "    fixed_terms = [term.strip() for term in rhs.split(\"+\") if term.strip() and term.strip() != \"-1\"]\n",
    "    # Always include an intercept if not explicitly removed:\n",
    "    fixed_colnames = [\"Intercept\"] + fixed_terms\n",
    "\n",
    "    # Build fixed effects design matrix X.\n",
    "    n = data.shape[0]\n",
    "    # Create intercept column\n",
    "    intercept = np.ones((n, 1))\n",
    "    if fixed_terms:\n",
    "        X_fixed = data[fixed_terms].values  # assume these columns exist in data\n",
    "        X = np.hstack([intercept, X_fixed])\n",
    "    else:\n",
    "        X = intercept.copy()\n",
    "    # Get response vector y.\n",
    "    y = data[response].values\n",
    "    groups = None\n",
    "    random_effect_cols = []  # list of indices in fixed design matrix to use as random slopes.\n",
    "    \n",
    "    if rand_term is not None:\n",
    "        # rand_term should be of the form \"1 + x2 | group\" or \"x1 | group\"\n",
    "        # Split at '|'\n",
    "        parts = rand_term.split(\"|\")\n",
    "        if len(parts) != 2:\n",
    "            raise ValueError(\"Random effects term not in expected format: '(...|group)'\")\n",
    "        rand_predictors = parts[0].strip()  # e.g., \"1 + x2\"\n",
    "        group_var = parts[1].strip()        # e.g., \"group\"\n",
    "        # Split rand_predictors by '+'\n",
    "        tokens = [tok.strip() for tok in rand_predictors.split(\"+\") if tok.strip()]\n",
    "        # For each token, map to an index in fixed_colnames.\n",
    "        for tok in tokens:\n",
    "            if tok == \"1\":\n",
    "                # random intercept corresponds to intercept column index 0.\n",
    "                if 0 not in random_effect_cols:\n",
    "                    random_effect_cols.append(0)\n",
    "            else:\n",
    "                if tok in fixed_colnames:\n",
    "                    idx = fixed_colnames.index(tok)\n",
    "                    random_effect_cols.append(idx)\n",
    "                else:\n",
    "                    raise ValueError(f\"Random effect term '{tok}' not found among fixed effects: {fixed_colnames}\")\n",
    "        # Get groups from the data:\n",
    "        groups = data[group_var].values\n",
    "    else:\n",
    "        # If no random term, default is random intercept (index 0) and groups must be provided separately.\n",
    "        raise ValueError(\"No random effects term found in the formula. Please include at least a random intercept term using (1|group).\")\n",
    "        \n",
    "    return X, y, groups, random_effect_cols, fixed_colnames\n",
    "\n",
    "# -----------------------\n",
    "# Updated GLMM Class with R-formula interface and Regularization options\n",
    "# -----------------------\n",
    "class GLMM:\n",
    "    def _init_(self, X=None, y=None, groups=None, distribution='gaussian', link='identity',\n",
    "                 random_effect_cols=None, formula=None, data=None, regularization=None, reg_lambda=0.0):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "         - Either provide X (n x p numpy array or DataFrame), y (n,), groups, and random_effect_cols (list of indices)\n",
    "           OR supply a formula (R-style string) and a pandas DataFrame (data).\n",
    "         - distribution: one of 'gaussian', 'binomial', 'gamma', 'poisson'\n",
    "         - link: one of 'identity', 'log', 'logit'\n",
    "         - random_effect_cols: list of indices in fixed design matrix (if not using formula).\n",
    "         - formula: R-style formula (e.g., \"y ~ x1 + x2 + (1 + x2 | group)\")\n",
    "         - data: pandas DataFrame (required if formula is provided)\n",
    "         - regularization: None, 'L1', or 'L2'\n",
    "         - reg_lambda: regularization coefficient (nonnegative float)\n",
    "        \"\"\"\n",
    "        self.distribution = distribution.lower()\n",
    "        self.link = link.lower()\n",
    "        self.regularization = regularization\n",
    "        self.reg_lambda = reg_lambda\n",
    "        \n",
    "        if formula is not None and data is not None:\n",
    "            # Use our formula parser.\n",
    "            self.formula = formula # ------------------ ADD\n",
    "            X, y, groups, random_effect_cols, fixed_colnames = parse_formula(formula, data)\n",
    "            self.fixed_colnames = fixed_colnames\n",
    "        else:\n",
    "            if X is None or y is None or groups is None:\n",
    "                raise ValueError(\"Either supply (X, y, groups, random_effect_cols) or (formula and data)\")\n",
    "            # If X is provided as a DataFrame, get its column names.\n",
    "            if isinstance(X, pd.DataFrame):\n",
    "                self.fixed_colnames = list(X.columns)\n",
    "                X = X.values\n",
    "            else:\n",
    "                # If X is a numpy array, we assume columns are numbered.\n",
    "                self.fixed_colnames = [f\"X{i}\" for i in range(X.shape[1])]\n",
    "            random_effect_cols = random_effect_cols if random_effect_cols is not None else [0]\n",
    "        \n",
    "        # Store fixed effects design matrix and response.\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "        self.n, self.p = self.X.shape\n",
    "        \n",
    "        # Process groups.\n",
    "        groups = np.array(groups)\n",
    "        unique_groups, group_idx = np.unique(groups, return_inverse=True)\n",
    "        self.group_idx = torch.tensor(group_idx, dtype=torch.long)\n",
    "        self.n_groups = len(unique_groups)\n",
    "        self.unique_groups = unique_groups\n",
    "        \n",
    "        # Build random-effect design matrix Z.\n",
    "        # Z has d columns: always 1 for random intercept plus additional columns for each random slope.\n",
    "        self.random_effect_cols = random_effect_cols  # list of indices in fixed effect design matrix.\n",
    "        d = len(random_effect_cols)\n",
    "        self.d = d\n",
    "        n = self.n\n",
    "        # For each observation, Z_i: each column j is the fixed design matrix column at index random_effect_cols[j]\n",
    "        Z = np.zeros((n, d))\n",
    "        for j, col_idx in enumerate(random_effect_cols):\n",
    "            # For random intercept, the column should be ones.\n",
    "            if col_idx == 0:\n",
    "                Z[:, j] = 1.0\n",
    "            else:\n",
    "                Z[:, j] = self.X[:, col_idx].detach().numpy().ravel()\n",
    "        self.Z = torch.tensor(Z, dtype=torch.float32)\n",
    "        \n",
    "        # Initialize fixed effects beta (p x 1)\n",
    "        self.beta = nn.Parameter(torch.zeros(self.p, 1, dtype=torch.float32))\n",
    "        # Initialize random effects b for each group (n_groups x d)\n",
    "        self.b = nn.Parameter(torch.zeros(self.n_groups, d, dtype=torch.float32))\n",
    "        # Initialize random effects std dev per component via log_sigma_b (d-dimensional vector)\n",
    "        self.log_sigma_b = nn.Parameter(torch.zeros(d, dtype=torch.float32))\n",
    "        \n",
    "        # For gaussian, also estimate residual sigma_y; for gamma, estimate shape phi.\n",
    "        if self.distribution == 'gaussian':\n",
    "            self.log_sigma_y = nn.Parameter(torch.tensor(0.0, dtype=torch.float32))\n",
    "        elif self.distribution == 'gamma':\n",
    "            self.log_phi = nn.Parameter(torch.tensor(0.0, dtype=torch.float32))\n",
    "            \n",
    "        # Collect parameters to optimize.\n",
    "        self.params = [self.beta, self.b, self.log_sigma_b]\n",
    "        if self.distribution == 'gaussian':\n",
    "            self.params.append(self.log_sigma_y)\n",
    "        elif self.distribution == 'gamma':\n",
    "            self.params.append(self.log_phi)\n",
    "            \n",
    "        self.loss_history = []\n",
    "\n",
    "    def model_loglik(self):\n",
    "        \"\"\"\n",
    "        Compute the negative joint log-likelihood:\n",
    "         - observation log-likelihood based on distribution and link.\n",
    "         - penalty for random effects assuming b_g ~ N(0, diag(sigma_b^2))\n",
    "         - plus optional regularization penalty on beta and b.\n",
    "        \"\"\"\n",
    "        b_obs = self.b[self.group_idx]  # shape (n, d)\n",
    "        # Compute contribution from random effects via design matrix Z:\n",
    "        rand_part = torch.sum(self.Z * b_obs, dim=1, keepdim=True)\n",
    "        eta = self.X @ self.beta + rand_part\n",
    "        mu = inverse_link(eta, self.link)\n",
    "        \n",
    "        if self.distribution == 'gaussian':\n",
    "            sigma_y = torch.exp(self.log_sigma_y)\n",
    "            ll_obs = loglik_gaussian(self.y, mu, sigma_y)\n",
    "        elif self.distribution == 'binomial':\n",
    "            ll_obs = loglik_binomial(self.y, mu)\n",
    "        elif self.distribution == 'poisson':\n",
    "            ll_obs = loglik_poisson(self.y, mu)\n",
    "        elif self.distribution == 'gamma':\n",
    "            phi = torch.exp(self.log_phi)\n",
    "            ll_obs = loglik_gamma(self.y, mu, phi)\n",
    "        else:\n",
    "            raise ValueError(\"Distribution not recognized.\")\n",
    "        ll_obs_sum = torch.sum(ll_obs)\n",
    "        \n",
    "        # Random effects penalty: each b_{g,j} ~ N(0, sigma_b[j]^2)\n",
    "        sigma_b = torch.exp(self.log_sigma_b)  # shape (d,)\n",
    "        ll_rand = -0.5 * torch.sum((self.b*2) / (sigma_b*2)) \\\n",
    "                  - self.n_groups * torch.sum(torch.log(sigma_b)) \\\n",
    "                  - 0.5 * self.n_groups * self.d * math.log(2 * math.pi)\n",
    "                  \n",
    "        total_ll = ll_obs_sum + ll_rand\n",
    "        \n",
    "        # Regularization penalty (if any) on fixed effects and random effects:\n",
    "        reg_penalty = 0.0\n",
    "        if self.regularization is not None and self.reg_lambda > 0:\n",
    "            if self.regularization.lower() == 'l1':\n",
    "                reg_penalty = self.reg_lambda * (torch.sum(torch.abs(self.beta)) + torch.sum(torch.abs(self.b)))\n",
    "            elif self.regularization.lower() == 'l2':\n",
    "                reg_penalty = self.reg_lambda * (torch.sum(self.beta*2) + torch.sum(self.b*2))\n",
    "            else:\n",
    "                raise ValueError(\"regularization must be either 'L1', 'L2', or None\")\n",
    "        \n",
    "        return -total_ll + reg_penalty\n",
    "\n",
    "    def fit(self, lr=0.01, epochs=5000, verbose=True):\n",
    "        optimizer = optim.Adam(self.params, lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.model_loglik()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            self.loss_history.append(loss.item())\n",
    "            if verbose and ((epoch % 500 == 0) or (epoch == epochs-1)):\n",
    "                print(f\"Epoch {epoch}: Negative Log-Likelihood = {loss.item():.4f}\")\n",
    "                \n",
    "    # def predict(self, new_data=None, X_new = None, groups_new = None):# ------------------ CHANGE (ADD new_data)\n",
    "    #     \"\"\"\n",
    "    #     Predict on new data.\n",
    "    #     new_data is optional. If provided, it should be a DataFrame with the same columns as used in training.\n",
    "    #     X_new: fixed-effects design matrix (numpy array or DataFrame) with same columns as used in training.\n",
    "    #     groups_new: group labels for each observation.\n",
    "    #     For new groups (not seen in training), random effects are set to 0.\n",
    "    #     \"\"\"\n",
    "    #     print(1)\n",
    "    #     if self.formula is not None and new_data is not None:# ------------------ ADD\n",
    "    #         X_new, y, groups_new, random_effect_cols, fixed_colnames = parse_formula(self.formula, new_data)# ------------------ ADD\n",
    "    #     elif isinstance(X_new, pd.DataFrame):# ------------------ CHANGE (if to elif)\n",
    "    #         X_new = X_new[self.fixed_colnames[1:]]  # skip intercept column name\n",
    "    #         # Prepend a column of ones for intercept.\n",
    "    #         X_new = np.hstack([np.ones((X_new.shape[0], 1)), X_new.values])\n",
    "    #     else:\n",
    "    #         X_new = np.array(X_new)\n",
    "    #     print(2)\n",
    "    #     X_new = torch.tensor(X_new, dtype=torch.float32)\n",
    "    #     n_new = X_new.shape[0]\n",
    "    #     # Build new Z matrix using the same random_effect_cols.\n",
    "    #     d = self.d\n",
    "    #     Z_new = np.zeros((n_new, d))\n",
    "    #     print(3)\n",
    "    #     for j, col_idx in enumerate(self.random_effect_cols):\n",
    "    #         if col_idx == 0:\n",
    "    #             Z_new[:, j] = 1.0\n",
    "    #         else:\n",
    "    #             Z_new[:, j] = X_new[:, col_idx].detach().numpy().ravel()\n",
    "    #     print(4)\n",
    "    #     Z_new = torch.tensor(Z_new, dtype=torch.float32)\n",
    "    #     # Map groups_new to indices.\n",
    "    #     b_new_list = []\n",
    "    #     print(5)\n",
    "    #     for g in groups_new:\n",
    "    #         print(g)\n",
    "    #         if g in self.unique_groups:\n",
    "    #             print('ini')\n",
    "    #             idx = np.where(self.unique_groups == g)[0][0]\n",
    "    #             b_new_list.append(self.b[idx])\n",
    "    #             print('fim')\n",
    "    #         else:\n",
    "    #             b_new_list.append(torch.zeros(self.d, dtype=torch.float32))\n",
    "    #     print(6)\n",
    "    #     b_new = torch.stack(b_new_list)  # shape (n_new, d)\n",
    "    #     eta_new = X_new @ self.beta.detach() + torch.sum(Z_new * b_new, dim=1, keepdim=True)\n",
    "    #     mu_new = inverse_link(eta_new, self.link)\n",
    "    #     print(7)\n",
    "    #     return mu_new.detach().numpy()\n",
    "\n",
    "    def predict(self, new_data=None, X_new = None, groups_new = None):# ------------------ CHANGE (ADD new_data)\n",
    "        \"\"\"\n",
    "        Predict on new data.\n",
    "        new_data is optional. If provided, it should be a DataFrame with the same columns as used in training.\n",
    "        X_new: fixed-effects design matrix (numpy array or DataFrame) with same columns as used in training.\n",
    "        groups_new: group labels for each observation.\n",
    "        For new groups (not seen in training), random effects are set to 0.\n",
    "        \"\"\"\n",
    "        if self.formula is not None and new_data is not None:# ------------------ ADD\n",
    "            X_new, y, groups_new, random_effect_cols, fixed_colnames = parse_formula(self.formula, new_data)# ------------------ ADD\n",
    "        elif isinstance(X_new, pd.DataFrame):# ------------------ CHANGE (if to elif)\n",
    "            X_new = X_new[self.fixed_colnames[1:]]  # skip intercept column name\n",
    "            # Prepend a column of ones for intercept.\n",
    "            X_new = np.hstack([np.ones((X_new.shape[0], 1)), X_new.values])\n",
    "        else:\n",
    "            X_new = np.array(X_new)\n",
    "        X_new = torch.tensor(X_new, dtype=torch.float32)\n",
    "        n_new = X_new.shape[0]\n",
    "        # Build new Z matrix using the same random_effect_cols.\n",
    "        d = self.d\n",
    "        Z_new = np.zeros((n_new, d))\n",
    "        for j, col_idx in enumerate(self.random_effect_cols):\n",
    "            if col_idx == 0:\n",
    "                Z_new[:, j] = 1.0\n",
    "            else:\n",
    "                Z_new[:, j] = X_new[:, col_idx].detach().numpy().ravel()\n",
    "        Z_new = torch.tensor(Z_new, dtype=torch.float32)\n",
    "        # Create a mapping from group label to index\n",
    "        group_to_index = {group: i for i, group in enumerate(self.unique_groups)}\n",
    "\n",
    "        # Convert groups_new to indices, using -1 for unseen groups\n",
    "        group_indices = torch.tensor([\n",
    "            group_to_index.get(g, -1) for g in groups_new\n",
    "        ], dtype=torch.long)\n",
    "\n",
    "        # Mask for seen/unseen groups\n",
    "        seen_mask = group_indices != -1\n",
    "\n",
    "        # Allocate zero tensor for all b_new\n",
    "        b_new = torch.zeros((len(groups_new), self.d), dtype=torch.float32)\n",
    "\n",
    "        # Fill in values for seen groups\n",
    "        seen_indices = group_indices[seen_mask]\n",
    "        b_new[seen_mask] = self.b[seen_indices]\n",
    "\n",
    "        eta_new = X_new @ self.beta.detach() + torch.sum(Z_new * b_new, dim=1, keepdim=True)\n",
    "        mu_new = inverse_link(eta_new, self.link)\n",
    "        return mu_new.detach().numpy()\n",
    "\n",
    "    def summary(self):\n",
    "        print(\"Fixed Effects (beta):\")\n",
    "        print(self.beta.detach().numpy().ravel())\n",
    "        print(\"Random Effects (first 5 groups):\")\n",
    "        print(self.b.detach().numpy()[:5])\n",
    "        sigma_b = torch.exp(self.log_sigma_b).detach().numpy()\n",
    "        print(\"Random Effects Std Dev (sigma_b) per component:\")\n",
    "        print(sigma_b)\n",
    "        if self.distribution == 'gaussian':\n",
    "            sigma_y = torch.exp(self.log_sigma_y).item()\n",
    "            print(f\"Residual Std Dev (sigma_y): {sigma_y:.4f}\")\n",
    "        elif self.distribution == 'gamma':\n",
    "            phi = torch.exp(self.log_phi).item()\n",
    "            print(f\"Gamma shape (phi): {phi:.4f}\")\n",
    "\n",
    "    def plot_loss(self):\n",
    "        plt.plot(self.loss_history)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Negative Log-Likelihood\")\n",
    "        plt.title(\"Loss History\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_residuals(self):\n",
    "        with torch.no_grad():\n",
    "            b_obs = self.b[self.group_idx]\n",
    "            rand_part = torch.sum(self.Z * b_obs, dim=1, keepdim=True)\n",
    "            eta = self.X @ self.beta + rand_part\n",
    "            mu = inverse_link(eta, self.link)\n",
    "            residuals = self.y - mu\n",
    "        fitted = mu.detach().numpy().ravel()\n",
    "        resid = residuals.detach().numpy().ravel()\n",
    "        plt.scatter(fitted, resid, alpha=0.6)\n",
    "        plt.xlabel(\"Fitted values\")\n",
    "        plt.ylabel(\"Residuals\")\n",
    "        plt.title(\"Residuals vs Fitted\")\n",
    "        plt.axhline(0, color='red', linestyle='--')\n",
    "        plt.show()\n",
    "\n",
    "# ---------------------------\n",
    "# Example: Simulate Data with Random Intercept and Slope (Gaussian GLMM)\n",
    "# ---------------------------\n",
    "def simulate_gaussian_glmm_random_slopes(n_groups=20, group_size=30, beta_true=[1.0, -0.5],\n",
    "                                         sigma_b_intercept=1.0, sigma_b_slope=0.5,\n",
    "                                         sigma_y_true=0.5, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    n = n_groups * group_size\n",
    "    # Fixed effects: intercept and one predictor \"x\"\n",
    "    X = np.ones((n, 2))\n",
    "    X[:,1] = np.random.normal(0, 1, size=n)\n",
    "    groups = np.repeat(np.arange(n_groups), group_size)\n",
    "    b_intercepts = np.random.normal(0, sigma_b_intercept, size=n_groups)\n",
    "    b_slopes = np.random.normal(0, sigma_b_slope, size=n_groups)\n",
    "    eta = beta_true[0] + beta_true[1]*X[:,1] + b_intercepts[groups] + b_slopes[groups]*X[:,1]\n",
    "    y = eta + np.random.normal(0, sigma_y_true, size=(n, 1))\n",
    "    return X, y.ravel(), groups"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Classe",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}